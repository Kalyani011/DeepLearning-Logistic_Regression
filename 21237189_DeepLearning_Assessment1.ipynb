{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Assignment 1\n",
    "\n",
    "#### Submitted By: Kalyani Prashant Kawale\n",
    "#### Student ID: 21237189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressor(X, y, alpha, max_iters):\n",
    "    (nsamples, nattributes) = np.shape(X)\n",
    "    threshold = 1e-6\n",
    "    w = np.random.rand(nattributes)\n",
    "    b = np.random.rand()\n",
    "    J_prev = 0\n",
    "    \n",
    "    for i in range(max_iters):   \n",
    "        idx = np.random.choice(nsamples, 1, replace=False)\n",
    "        random_X = X[idx[0]]\n",
    "        random_y = y[idx[0]]\n",
    "        y_hat = 1 / (1 + np.exp(-1 * (np.dot(w, random_X) + b)))\n",
    "        J_curr = -1 * ((random_y * np.log(y_hat)) + ((1 - random_y) * np.log(1 - y_hat)))\n",
    "        if np.absolute(J_curr - J_prev) < threshold:\n",
    "            break\n",
    "        else:\n",
    "            J_prev = J_curr        \n",
    "        delta_w = []\n",
    "        for j in range(len(w)):\n",
    "            delta_w.append((y_hat - random_y) * random_X[j])\n",
    "        delta_b = (y_hat - random_y)\n",
    "        for j in range(len(w)):\n",
    "            w[j] -= alpha * delta_w[j]\n",
    "        b -= alpha * delta_b\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "def predict(data, weights, bias):\n",
    "    predictions = []\n",
    "    for sample in data:\n",
    "        prediction = 1 / (1 + np.exp(-1 * (np.dot(weights, sample) + bias))) \n",
    "        if prediction >= 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Training, Validating, Testing on blobs300 and circles600 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"blobs300.csv\", \"circles600.csv\"]\n",
    "alpha_s = [0.1, 0.001, 0.0001]\n",
    "iterations = [1000, 10000, 20000]\n",
    "best_alpha = 0.1\n",
    "best_iterations = 1000\n",
    "best_accuracy = 0\n",
    "X_train = {}\n",
    "y_train = {}\n",
    "X_valid = {}\n",
    "y_valid = {}\n",
    "X_test = {}\n",
    "y_test = {}\n",
    "results = pd.DataFrame(columns=['File Name', 'Learning Rate', 'Iterations', 'Accuracy'])\n",
    "np.random.seed(200)\n",
    "for file in files:\n",
    "    # Use pandas to read the CSV file as a dataframe\n",
    "    df = pd.read_csv(file).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    \n",
    "    X_train[file], X_validate_test, y_train[file], y_validate_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "    X_valid[file], X_test[file], y_valid[file], y_test[file] = train_test_split(X_validate_test, y_validate_test, test_size=0.5)\n",
    "    \n",
    "    for alpha, iters in itertools.product(alpha_s, iterations):\n",
    "        row = {'File Name': file, 'Learning Rate': alpha, 'Iterations': iters}\n",
    "        w, b = LogisticRegressor(X_train[file], y_train[file], alpha, iters)\n",
    "        predictions = predict(X_valid[file], w, b)\n",
    "        accuracy = accuracy_score(y_valid[file], predictions)\n",
    "        row['Accuracy'] = accuracy\n",
    "        results = results.append(row, ignore_index=True)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_alpha = alpha\n",
    "            best_iterations = iters\n",
    "            best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training and Validation Results:\\n\")\n",
    "print(results.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "print(\"Testing Results:\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "    w, b = LogisticRegressor(X_train[file], y_train[file], best_alpha, best_iterations)\n",
    "    predictions = predict(X_test[file], w, b)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test[file], predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Shallow Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-1 * z))\n",
    "\n",
    "def f_dash(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def ForwardPropagation(x, weights, biases):\n",
    "    sigmas = []\n",
    "    activations = []\n",
    "    # Initialising activations with input values for layer 1\n",
    "    activations.append(x)    \n",
    "    for layer in range(len(weights)):\n",
    "#         print(f\"Layer {layer}\")\n",
    "#         print(weights[layer].shape)\n",
    "        sigma_layer = np.zeros(weights[layer].shape[0])\n",
    "        activations_layer = np.zeros(weights[layer].shape[0])\n",
    "        for i in range(weights[layer].shape[0]):            \n",
    "            sigma = 0\n",
    "            for j in range(weights[layer].shape[1]):\n",
    "                sigma += weights[layer][i][j] * activations[layer][j]\n",
    "            sigma_layer[i] = sigma + biases[layer][i]\n",
    "            activations_layer[i] = f(sigma_layer[i])\n",
    "        sigmas.append(sigma_layer)\n",
    "        activations.append(activations_layer)\n",
    "            \n",
    "    return sigmas, activations\n",
    "\n",
    "def BackPropagation(y, activations, sigmas, weights):\n",
    "    delta_weights = []\n",
    "    delta_biases = []\n",
    "    delta_sigmas = [activations[len(activations)-1] - y]\n",
    "#     print(f\"delta_sigmas {delta_sigmas}\")\n",
    "    for i in range(weights[len(weights) - 1].shape[0]):\n",
    "        delta_weight = []\n",
    "        for j in range(weights[len(weights) - 1].shape[1]):\n",
    "            delta_weight.append(delta_sigmas[i][0] * activations[len(activations)-2][j])\n",
    "        delta_weights.append(np.array([delta_weight]))\n",
    "        \n",
    "    for layer in range(len(weights) - 1):\n",
    "#         print(f\"at layer {layer}\")\n",
    "        # first finding the delta sigmas\n",
    "        delta_sigma_layer = np.zeros(sigmas[layer].shape[0])\n",
    "        for i in range(sigmas[layer].shape[0]):\n",
    "#             print(f\"sigma at layer {layer} and {i}th position {sigmas[layer][i]}\")\n",
    "            delta_sigma = f_dash(sigmas[layer][i])            \n",
    "            summation = 0            \n",
    "            for j in range(weights[layer + 1].shape[0]):\n",
    "                for k in range(weights[layer + 1].shape[1]):\n",
    "                    summation += weights[layer + 1][j][k] * delta_sigmas[layer - 1][j]      \n",
    "            \n",
    "            delta_sigma_layer[i] = delta_sigma * summation\n",
    "        delta_sigmas.append(delta_sigma_layer)\n",
    "        \n",
    "        delta_layer_weights = np.zeros(weights[layer].shape)\n",
    "        for i in range(weights[layer].shape[0]):\n",
    "            for j in range(weights[layer].shape[1]):\n",
    "                delta_layer_weights[i][j] = delta_sigmas[layer + 1][i] * activations[layer][j]\n",
    "        delta_weights.append(delta_layer_weights)\n",
    "    \n",
    "    delta_biases = delta_sigmas\n",
    "                \n",
    "    return list(reversed(delta_weights)), list(reversed(delta_biases))\n",
    "\n",
    "def SGD_Parameter_Update(alpha, weights, biases, delta_weights, delta_biases):\n",
    "    \n",
    "    for layer in range(len(weights)):\n",
    "        for i in range(weights[layer].shape[0]):    \n",
    "            for j in range(weights[layer].shape[1]):\n",
    "                weights[layer][i][j] -= alpha * delta_weights[layer][i][j]\n",
    "    \n",
    "    for layer in range(len(biases)):\n",
    "        for i in range(biases[layer].shape[0]):\n",
    "            biases[layer][i] -= alpha * delta_biases[layer][i]\n",
    "    \n",
    "    return weights, biases\n",
    "                \n",
    "def predictNN(data, weights, biases):\n",
    "    predictions = []\n",
    "    for sample in data:\n",
    "        sigmas, activations = ForwardPropagation(sample, weights, biases)\n",
    "        prediction = activations[len(activations) - 1][0]\n",
    "        if prediction > 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "    \n",
    "def LogisticRegressor_1NN(X, y, alpha, max_iters, layers=2, hidden_layer_neurons=3):\n",
    "    errors = []\n",
    "    loss = []\n",
    "    (nsamples, nattributes) = np.shape(X)\n",
    "    threshold = 1e-6\n",
    "    \n",
    "    layer_neurons = []\n",
    "    for layer in range(layers-1):\n",
    "        layer_neurons.append(hidden_layer_neurons)\n",
    "    layer_neurons.append(1)\n",
    "    layer_neurons.insert(0, nattributes)\n",
    "    \n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, layers + 1):\n",
    "        weights.append(np.random.rand(layer_neurons[i], layer_neurons[i - 1]))\n",
    "        biases.append(np.array(np.random.rand(layer_neurons[i])))\n",
    "    print(\"weights\")\n",
    "    print(weights)\n",
    "    print(\"Biases\")\n",
    "    print(biases)\n",
    "    \n",
    "    J_prev = 0\n",
    "        \n",
    "    for iters in range(max_iters):\n",
    "        idx = np.random.choice(nsamples, 1, replace=False)\n",
    "        random_X = X[idx[0]]\n",
    "        random_y = y[idx[0]]\n",
    "        sigmas, activations = ForwardPropagation(random_X, weights, biases)\n",
    "\n",
    "        y_hat = activations[len(activations)-1][0]        \n",
    "        J_curr = -1 * ((random_y * np.log(y_hat)) + ((1 - random_y) * np.log(1 - y_hat)))        \n",
    "        delta_weights, delta_biases = BackPropagation(random_y, activations, sigmas, weights)\n",
    "        if np.absolute(J_curr - J_prev) < threshold:\n",
    "            break\n",
    "        else:\n",
    "            J_prev = J_curr\n",
    "        \n",
    "        weights, biases = SGD_Parameter_Update(alpha, weights, biases, delta_weights, delta_biases)\n",
    "            \n",
    "    return weights, biases, errors, loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "[array([[0.78863355, 0.84340158, 0.07328244, 0.75937946],\n",
      "       [0.04413816, 0.10103551, 0.97446269, 0.51075187],\n",
      "       [0.84742544, 0.93746915, 0.62025996, 0.3543248 ],\n",
      "       [0.43755636, 0.40304762, 0.99846879, 0.83245862]]), array([[0.28449671, 0.12503889, 0.9858171 , 0.84251261]])]\n",
      "Biases\n",
      "[array([0.59418336, 0.3475767 , 0.49891584, 0.65814956]), array([0.77493989])]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"blobs300.csv\")\n",
    "# df = pd.read_csv(\"circles600.csv\")\n",
    "y = df['Class'].values\n",
    "del df['Class']\n",
    "X = df.values   \n",
    "# np.random.seed(200)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7)\n",
    "(nsamples, nattributes) = X_train.shape\n",
    "#300\n",
    "# weights, biases, errors, loss = LogisticRegressor_1NN(X_train, y_train, 0.1, 2000, layers=2, hidden_layer_neurons=int(nattributes / 2))\n",
    "# 600\n",
    "weights, biases, errors, loss = LogisticRegressor_1NN(X_train, y_train, 0.1, 10000, layers=2, hidden_layer_neurons=4)\n",
    "# print(\"UPDATED WEIGHTS AND BIASES\")\n",
    "# print(weights)\n",
    "# print(biases)\n",
    "\n",
    "predictions = predictNN(X_valid, weights, biases)\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
