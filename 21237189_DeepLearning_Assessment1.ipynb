{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Assignment 1\n",
    "\n",
    "#### Submitted By: Kalyani Prashant Kawale\n",
    "#### Student ID: 21237189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(200)\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressor(X, y, alpha, max_iters):\n",
    "    (nsamples, nattributes) = np.shape(X)\n",
    "    threshold = 1e-6\n",
    "    w = np.random.rand(nattributes)\n",
    "    b = np.random.rand()\n",
    "    J_prev = 0\n",
    "    i = 0\n",
    "    for i in range(max_iters):   \n",
    "        idx = np.random.choice(nsamples, 1, replace=False)\n",
    "        random_X = X[idx[0]]\n",
    "        random_y = y[idx[0]]\n",
    "        y_hat = 1 / (1 + np.exp(-1 * (np.dot(w, random_X) + b)))\n",
    "        J_curr = -1 * ((random_y * np.log(y_hat)) + ((1 - random_y) * np.log(1 - y_hat)))\n",
    "        if np.absolute(J_curr - J_prev) < threshold:\n",
    "            break\n",
    "        else:\n",
    "            J_prev = J_curr        \n",
    "        delta_w = []\n",
    "        for j in range(len(w)):\n",
    "            delta_w.append((y_hat - random_y) * random_X[j])\n",
    "        delta_b = (y_hat - random_y)\n",
    "        for j in range(len(w)):\n",
    "            w[j] -= alpha * delta_w[j]\n",
    "        b -= alpha * delta_b\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "def predict(data, weights, bias):\n",
    "    predictions = []\n",
    "    for sample in data:\n",
    "        prediction = 1 / (1 + np.exp(-1 * (np.dot(weights, sample) + bias))) \n",
    "        if prediction >= 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Training, Validating, Testing on blobs300 and circles600 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"blobs300.csv\", \"circles600.csv\"]\n",
    "alpha_s = [0.1, 0.001, 0.0001]\n",
    "iterations = [1000, 10000, 20000]\n",
    "best_alpha = 0.1\n",
    "best_iterations = 1000\n",
    "best_accuracy = 0\n",
    "X_train = {}\n",
    "y_train = {}\n",
    "X_valid = {}\n",
    "y_valid = {}\n",
    "X_test = {}\n",
    "y_test = {}\n",
    "results = pd.DataFrame(columns=['File Name', 'Learning Rate', 'Iterations', 'Accuracy'])\n",
    "for file in files:\n",
    "    # Use pandas to read the CSV file as a dataframe\n",
    "    df = pd.read_csv(file).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    \n",
    "    X_train[file], X_validate_test, y_train[file], y_validate_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "    X_valid[file], X_test[file], y_valid[file], y_test[file] = train_test_split(X_validate_test, y_validate_test, test_size=0.5)\n",
    "    \n",
    "    for alpha, iters in itertools.product(alpha_s, iterations):\n",
    "        row = {'File Name': file, 'Learning Rate': alpha, 'Iterations': iters}\n",
    "        w, b = LogisticRegressor(X_train[file], y_train[file], alpha, iters)\n",
    "        predictions = predict(X_valid[file], w, b)\n",
    "        accuracy = accuracy_score(y_valid[file], predictions)\n",
    "        row['Accuracy'] = accuracy\n",
    "        results = results.append(row, ignore_index=True)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_alpha = alpha\n",
    "            best_iterations = iters\n",
    "            best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation Results:\n",
      "\n",
      "|    | File Name      |   Learning Rate |   Iterations |   Accuracy |\n",
      "|---:|:---------------|----------------:|-------------:|-----------:|\n",
      "|  0 | blobs300.csv   |          0.1    |         1000 |   1        |\n",
      "|  1 | blobs300.csv   |          0.1    |        10000 |   1        |\n",
      "|  2 | blobs300.csv   |          0.1    |        20000 |   1        |\n",
      "|  3 | blobs300.csv   |          0.001  |         1000 |   0.933333 |\n",
      "|  4 | blobs300.csv   |          0.001  |        10000 |   1        |\n",
      "|  5 | blobs300.csv   |          0.001  |        20000 |   0.977778 |\n",
      "|  6 | blobs300.csv   |          0.0001 |         1000 |   0.577778 |\n",
      "|  7 | blobs300.csv   |          0.0001 |        10000 |   0.577778 |\n",
      "|  8 | blobs300.csv   |          0.0001 |        20000 |   0.911111 |\n",
      "|  9 | circles600.csv |          0.1    |         1000 |   0.655556 |\n",
      "| 10 | circles600.csv |          0.1    |        10000 |   0.477778 |\n",
      "| 11 | circles600.csv |          0.1    |        20000 |   0.677778 |\n",
      "| 12 | circles600.csv |          0.001  |         1000 |   0.522222 |\n",
      "| 13 | circles600.csv |          0.001  |        10000 |   0.588889 |\n",
      "| 14 | circles600.csv |          0.001  |        20000 |   0.333333 |\n",
      "| 15 | circles600.csv |          0.0001 |         1000 |   0.7      |\n",
      "| 16 | circles600.csv |          0.0001 |        10000 |   0.711111 |\n",
      "| 17 | circles600.csv |          0.0001 |        20000 |   0.522222 |\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and Validation Results:\\n\")\n",
    "print(results.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results:\n",
      "blobs300.csv\n",
      "Accuracy: 1.0\n",
      "circles600.csv\n",
      "Accuracy: 0.6777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Results:\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "    w, b = LogisticRegressor(X_train[file], y_train[file], best_alpha, best_iterations)\n",
    "    predictions = predict(X_test[file], w, b)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test[file], predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Shallow Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(z):\n",
    "#     return 1 / (1 + np.exp(-1 * z))\n",
    "\n",
    "# def LogisticRegressorNN(X, y, alpha, max_iters):\n",
    "#     (nsamples, nattributes) = np.shape(X)\n",
    "# #     hidden_layer_nodes = int(nattributes/2)\n",
    "#     hidden_layer_nodes = 2\n",
    "#     threshold = 1e-6\n",
    "#     # Initialising weights and biases for input to hidden layer\n",
    "#     w_H = np.random.rand(hidden_layer_nodes, nattributes)\n",
    "#     b_H = np.random.rand(hidden_layer_nodes)\n",
    "#     # Initialising weights and biases for hidden to output layer\n",
    "#     w_L = np.random.rand(hidden_layer_nodes)\n",
    "#     b_L = np.random.rand()\n",
    "#     J_prev = 0\n",
    "#     # Iterating until convergence\n",
    "#     for x in range(max_iters):\n",
    "#         # Selecting a random sample from dataset\n",
    "#         idx = np.random.choice(nsamples, 1, replace=False)\n",
    "#         random_X = X[idx[0]]\n",
    "#         random_y = y[idx[0]]\n",
    "        \n",
    "#         # Forward Propogation\n",
    "#         z_H = []\n",
    "#         a_H = []\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             temp = 0\n",
    "#             for j in range(nattributes):\n",
    "#                 temp += (w_H[i][j] * random_X[j]) + b_H[i]\n",
    "#             z_H.append(temp)\n",
    "#             a_H.append(f(z_H[i]))\n",
    "        \n",
    "#         z_L = 0\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             z_L += (w_L[i] * a_H[i]) + b_L\n",
    "#         y_hat = f(z_L)\n",
    "#         J_curr = -1 * ((random_y * np.log(y_hat)) + ((1 - random_y) * np.log(1 - y_hat)))\n",
    "        \n",
    "#         # Backward Propogation        \n",
    "#         # Propogating error back from output to hidden layer\n",
    "#         delta_z_L = y_hat - random_y\n",
    "#         delta_w_L = []\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             delta_w_L.append(delta_z_L * a_H[i])\n",
    "#         delta_b_L = delta_z_L\n",
    "        \n",
    "#         # Propogating error back from output to hidden layer\n",
    "#         delta_z_H = []        \n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             f_dash = f(z_H[i]) * (1 - f(z_H[i]))\n",
    "#             delta_z_H.append(f_dash * (delta_z_L * w_L[i]))\n",
    "\n",
    "#         delta_w_H = []\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             for j in range(nattributes):\n",
    "#                 delta_w_H.append(delta_z_H[i] * random_X[j])\n",
    "#         delta_b_H = delta_z_H\n",
    "        \n",
    "#         # Gradient Descent\n",
    "#         if np.absolute(J_curr - J_prev) < threshold:\n",
    "#             break\n",
    "#         else:\n",
    "#             J_prev = J_curr\n",
    "            \n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             w_L[i] -= alpha * delta_w_L[i]\n",
    "#         b_L -= alpha * delta_b_L\n",
    "        \n",
    "#         for i in range(hidden_layer_nodes):\n",
    "# #             for j in range(nattributes):                \n",
    "#             w_H[i] -= alpha * delta_w_H[i]\n",
    "#             b_H[i] -= alpha * delta_b_H[i]\n",
    "        \n",
    "#     return (w_H, b_H, w_L, b_L)\n",
    "\n",
    "# def predictNN(data, w_H, b_H, w_L, b_L):\n",
    "#     (nsamples, nattributes) = np.shape(X)\n",
    "#     hidden_layer_nodes = int(nattributes/2)\n",
    "#     probabilities = []\n",
    "#     for sample in data:\n",
    "#         z_H = []\n",
    "#         a_H = []\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             temp = 0\n",
    "#             for j in range(nattributes):\n",
    "#                 temp += (w_H[i][j] * sample[j]) + b_H[i]\n",
    "#             z_H.append(temp)\n",
    "#             a_H.append(f(z_H[i]))\n",
    "\n",
    "#         z_L = 0\n",
    "#         for i in range(hidden_layer_nodes):\n",
    "#             z_L += (w_L[i] * a_H[i]) + b_L\n",
    "#         probabilities.append(f(z_L))\n",
    "#     predictions = []\n",
    "#     for probability in probabilities:\n",
    "#         if probability > 0.5:\n",
    "#             predictions.append(1)\n",
    "#         else:\n",
    "#             predictions.append(0)\n",
    "#     return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"blobs300.csv\").sample(frac=1).reset_index(drop=True)\n",
    "# df = pd.read_csv(\"circles600.csv\").sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "y = df['Class'].values\n",
    "\n",
    "del df['Class']   # drop the 'Class' column from the dataframe\n",
    "X = df.values     # convert the remaining columns to a numpy array\n",
    "\n",
    "X_train, X_validate_test, y_train, y_validate_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_validate_test, y_validate_test, test_size=0.5)\n",
    "\n",
    "# w_H, b_H, w_L, b_L = LogisticRegressorNN(X_train, y_train, 0.1, 1000)\n",
    "\n",
    "# predictions = predictNN(X_valid, w_H, b_H, w_L, b_L)\n",
    "# print(f\"Accuracy: {accuracy_score(y_valid, predictions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3, l3\n",
      "l2\n",
      "2, l2\n",
      "l1\n",
      "1, l1\n",
      "l0\n",
      "{'l2': array([0.78757147, 0.91287139]), 'l1': array([0.78757147, 0.91287139, 0.26257361, 0.0910724 ]), 'l0': array([7.87571469e-01, 9.12871393e-01, 3.39888465e-04, 1.87986586e-03])}\n",
      "{'l0': array([[0.81158624, 0.04213984, 0.4090011 , 0.7304773 ],\n",
      "       [0.02843178, 0.9239733 , 0.6706423 , 0.04895458]]), 'l1': array([[0.71626912, 0.62057536],\n",
      "       [0.3574766 , 0.95100196]]), 'l2': array([0.89952034, 0.91887324])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (4,) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b713d12606c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mLogisticRegressorNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-b713d12606c7>\u001b[0m in \u001b[0;36mLogisticRegressorNN\u001b[1;34m(X, y, alpha, max_iters)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;31m#             print(\"eek\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;31m#             print(weights[layer])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta_biases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (4,) (2,2) "
     ]
    }
   ],
   "source": [
    "def Initialise(nattributes=0, n_layers=2):\n",
    "    # Initialising the threshold\n",
    "    threshold = 1e-6\n",
    "    # Setting number of layers, including output layer\n",
    "    layers = n_layers\n",
    "    # Setting the number of nodes in each hidden layer\n",
    "    layer_nodes = [int(nattributes/2) for layer in range(layers - 1)]\n",
    "    # Setting the number of nodes in output layer\n",
    "    layer_nodes.append(1)\n",
    "    # Initialising weights dictionary\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    # Initialising the weights matrix with the weights from input layer to first hidden layer\n",
    "    weights['l0'] = np.random.rand(layer_nodes[0], nattributes)\n",
    "    # Initialising the bias for each node of the first hidden layer\n",
    "    biases['l0'] = np.random.rand(layer_nodes[0])    \n",
    "    # Initialising weights and biases in each layer\n",
    "    for layer in range(layers - 1):\n",
    "        weights['l'+str(layer+1)] = np.squeeze(np.random.rand(layer_nodes[layer + 1], layer_nodes[layer]))\n",
    "        biases['l'+str(layer+1)] = np.squeeze(np.random.rand(layer_nodes[layer + 1]))\n",
    "        \n",
    "    return threshold, weights, biases\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-1 * z))\n",
    "\n",
    "def f_dash(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def ForwardPropogation(weights, biases, x):\n",
    "    # Forward Propogation\n",
    "    activations = {}\n",
    "    activations['l0'] = x\n",
    "    sigmas = {}\n",
    "    \n",
    "    layers = list(weights.keys())\n",
    "    last_weight_layer = layers[len(weights)-1]\n",
    "    \n",
    "    for idx, layer in enumerate(weights):\n",
    "        sigmas['l'+str(idx + 1)] = np.array([])\n",
    "        activations['l'+str(idx + 1)] = np.array([])\n",
    "        \n",
    "        if layer != last_weight_layer:\n",
    "            for i in range(len(weights[layer])):            \n",
    "                sigma = 0\n",
    "                for j in range(len(weights[layer][i])):\n",
    "                    sigma += (weights[layer][i][j] * activations[layer][j]) + biases[layer][i]                \n",
    "                sigmas['l'+str(idx + 1)] = np.append(sigmas['l'+str(idx + 1)], sigma)\n",
    "                activations['l'+str(idx + 1)] = np.append(activations['l'+str(idx + 1)], f(sigma))\n",
    "        else:\n",
    "            sigma = 0\n",
    "            for i in range(len(weights[layer])):\n",
    "                sigma += (weights[layer][i] * activations[layer][i]) + biases[layer]\n",
    "            sigmas['l'+str(idx + 1)] = np.append(sigmas['l'+str(idx + 1)], sigma)\n",
    "            activations['l'+str(idx + 1)] = np.append(activations['l'+str(idx + 1)], f(sigma))\n",
    "        \n",
    "    return activations, sigmas\n",
    "\n",
    "def BackPropagation(activations, sigmas, weights, y, layers, output_layer):    \n",
    "    delta_z = {}\n",
    "    delta_b = {}\n",
    "    delta_w = {}\n",
    "    # Calculating delta weights and delta bais for output layer\n",
    "    \n",
    "    delta_z[output_layer] = activations[output_layer] - y\n",
    "    last_weight_layer = layers[layers.index(output_layer) - 1]\n",
    "    delta_b[last_weight_layer] = delta_z[output_layer]    \n",
    "    delta_w[last_weight_layer] = np.array([])\n",
    "    for activation in activations[last_weight_layer]:\n",
    "        delta_w[last_weight_layer] = np.append(delta_w[last_weight_layer], delta_z[output_layer] * activation) \n",
    "    \n",
    "    # Calculating delta weights and delta bais for hidden layers   \n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(layers))):        \n",
    "        if layer != 'l0':\n",
    "            print(f\"{layer_idx}, {layer}\")\n",
    "\n",
    "            current_layer = layers[layer_idx - 1]\n",
    "            print(current_layer)\n",
    "            if current_layer != 'l0':\n",
    "                current_nodes = sigmas[current_layer]\n",
    "                delta_z[current_layer] = np.array([])               \n",
    "\n",
    "                for i in range(len(current_nodes)):\n",
    "                    _sum = 0\n",
    "                    try:\n",
    "                        for j in range(len(weights[current_layer][i])):\n",
    "                            _sum += delta_z[layer][j] * weights[current_layer][i][j]\n",
    "                        delta_sigma = f_dash(current_nodes[i]) * _sum                \n",
    "                        delta_z[current_layer] = np.append(delta_z[current_layer], delta_sigma)\n",
    "                    except:                    \n",
    "                        for j in range(len(weights[current_layer])):\n",
    "                            _sum += delta_z[layer] * weights[current_layer][j]\n",
    "                        delta_sigma = f_dash(current_nodes[i]) * _sum                \n",
    "                        delta_z[current_layer] = np.append(delta_z[current_layer], delta_sigma)\n",
    "        \n",
    "        ####NEEEDDDD TOOOO WOOOORRRKKK HEEERERERERE\n",
    "    for layer_idx, layer in reversed(list(enumerate(layers))):        \n",
    "        if layer != output_layer and layer != last_weight_layer:            \n",
    "            delta_w[layer] = np.array([])\n",
    "            delta_layer = layers[layer_idx + 1]            \n",
    "            for activation in activations[layer]:\n",
    "                node_delta_weights = np.array([])\n",
    "                for delta in delta_z[delta_layer]:\n",
    "                    node_delta_weights = np.append(node_delta_weights, delta * activation)\n",
    "                delta_w[layer] = np.append(delta_w[last_weight_layer], node_delta_weights) \n",
    "    \n",
    "            delta_b[layer] = delta_z[delta_layer]\n",
    "        \n",
    "    return delta_w, delta_b\n",
    "\n",
    "def LogisticRegressorNN(X, y, alpha, max_iters):\n",
    "    (nsamples, nattributes) = np.shape(X)\n",
    "    threshold, weights, biases, =  Initialise(nattributes=nattributes, n_layers=3)\n",
    "    J_prev = 0\n",
    "    max_iters = 1\n",
    "    # Iterating until convergence\n",
    "    for x in range(max_iters):\n",
    "        # Selecting a random sample from dataset\n",
    "        idx = np.random.choice(nsamples, 1, replace=False)\n",
    "        random_X = X[idx[0]]\n",
    "        random_y = y[idx[0]]\n",
    "        activations, sigmas = ForwardPropogation(weights, biases, random_X)\n",
    "        layers = list(activations.keys())\n",
    "        output_layer = layers[len(activations)-1]\n",
    "        y_hat = activations[output_layer]\n",
    "        J_curr = -1 * ((random_y * np.log(y_hat)) + ((1 - random_y) * np.log(1 - y_hat)))\n",
    "        delta_weights, delta_biases = BackPropagation(activations, sigmas, weights, random_y, layers, output_layer)\n",
    "        print(delta_weights)\n",
    "        print(weights)\n",
    "        # Gradient Descent\n",
    "        if np.absolute(J_curr - J_prev) < threshold:\n",
    "            break\n",
    "        else:\n",
    "            J_prev = J_curr\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer != output_layer:\n",
    "#             print(layer)\n",
    "#             print(delta_weights[layer])\n",
    "#             print(\"eek\")\n",
    "#             print(weights[layer])\n",
    "                weights[layer] -= alpha * delta_weights[layer]\n",
    "                biases[layer] -= alpha * delta_biases[layer]\n",
    "        \n",
    "\n",
    "LogisticRegressorNN(X_train, y_train, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
